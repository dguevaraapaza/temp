{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## L2-regularized linear regression via stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering the function designed in question 4\n",
    "import numpy as np\n",
    "\n",
    "def ridge_regression_sgd(X_train, y_train, X_val, y_val, alpha_vals, lr_vals, batch_sizes, epochs):\n",
    "    \"\"\"\n",
    "    Trains an L2-regularized linear regression model using SGD.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training features (n_samples, n_features)\n",
    "    - y_train: Training labels (n_samples,)\n",
    "    - X_val: Validation features (n_samples, n_features)\n",
    "    - y_val: Validation labels (n_samples,)\n",
    "    - alpha_vals: List of regularization strengths to try\n",
    "    - lr_vals: List of learning rates to try\n",
    "    - batch_sizes: List of batch sizes to try\n",
    "    - epochs: Number of epochs to run for each combination\n",
    "\n",
    "    Returns:\n",
    "    - Best weights and bias based on validation loss\n",
    "    \"\"\"\n",
    "\n",
    "    best_w, best_b = None, None\n",
    "    best_mse = float(\"inf\")\n",
    "    \n",
    "    for alpha in alpha_vals:\n",
    "        for lr in lr_vals:\n",
    "            for batch_size in batch_sizes:\n",
    "                # Initialize weights and bias\n",
    "                w = np.zeros(X_train.shape[1])\n",
    "                b = 0\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    # Shuffle data\n",
    "                    indices = np.arange(X_train.shape[0])\n",
    "                    np.random.shuffle(indices)\n",
    "                    X_train, y_train = X_train[indices], y_train[indices]\n",
    "                    \n",
    "                    # Mini-batch training\n",
    "                    for i in range(0, X_train.shape[0], batch_size):\n",
    "                        X_batch = X_train[i:i+batch_size]\n",
    "                        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "                        # Compute predictions\n",
    "                        preds = X_batch.dot(w) + b\n",
    "                        errors = preds - y_batch\n",
    "\n",
    "                        # Compute gradients\n",
    "                        grad_w = (2 / batch_size) * (X_batch.T.dot(errors)) + 2 * alpha * w\n",
    "                        grad_b = (2 / batch_size) * np.sum(errors)\n",
    "\n",
    "                        # Update parameters\n",
    "                        w -= lr * grad_w\n",
    "                        b -= lr * grad_b\n",
    "\n",
    "                # Compute validation loss (unregularized MSE)\n",
    "                val_preds = X_val.dot(w) + b\n",
    "                val_mse = np.mean((y_val - val_preds) ** 2)\n",
    "\n",
    "                # Update best parameters if current config is better\n",
    "                if val_mse < best_mse:\n",
    "                    best_mse = val_mse\n",
    "                    best_w, best_b = w, b\n",
    "\n",
    "    return best_w, best_b, best_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "X_train = np.load(\"data/age_regression_Xtr.npy\")\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)  # reshape to 2D\n",
    "y_train = np.load(\"data/age_regression_ytr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and validation sets (80/20)\n",
    "split_idx = int(0.8 * X_train.shape[0])\n",
    "X_val, y_val = X_train[split_idx:], y_train[split_idx:]\n",
    "X_train, y_train = X_train[:split_idx], y_train[:split_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter values\n",
    "alpha_vals = [0.01, 0.1, 1, 10]\n",
    "lr = [0.001, 0.01, 0.1]\n",
    "batch = [8, 16, 32]\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davo/miniconda3/envs/machine-learning/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/tmp/ipykernel_221890/1535244853.py:52: RuntimeWarning: invalid value encountered in subtract\n",
      "  w -= lr * grad_w\n",
      "/tmp/ipykernel_221890/1535244853.py:53: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  b -= lr * grad_b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights: [ 1.13468049  0.75873297  0.4128094  ... -0.25656862 -0.18807008\n",
      " -0.2141979 ]\n",
      "Best bias: 36.5783102349734\n",
      "Best validation MSE: 173.18378906888458\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "best_w, best_b, best_mse = ridge_regression_sgd(X_train, y_train, X_val, y_val, alpha_vals, lr, batch, epochs)\n",
    "\n",
    "# Print best results\n",
    "print(\"Best weights:\", best_w)\n",
    "print(\"Best bias:\", best_b)\n",
    "print(\"Best validation MSE:\", best_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
